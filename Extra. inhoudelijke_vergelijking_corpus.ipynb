{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra. Berekenen van de cosinusgelijkenis tussen documenten (document vectors)\n",
    "\n",
    "Het doel van dit script is om te berekenen in hoeverre documenten binnen een corpus met elkaar overeenkomen. Dit wordt gedaan aan de hand van de zogeheten cosinusgelijkenis. Twee identieke documenten krijgen een waarde van 1 toegewezenn. Twee documenten die veel samenhang vertonen krijgen een waarde die dichtbij de 1 ligt, terwijl twee documenten met minder samenhang een waarde krijgen die dichterbij de 0 ligt. \n",
    "\n",
    "Op basis van de uitkomst van de cosinusgelijkenis kunnen identieke documenten uit een corpus worden gehaald, ook al hebben ze afwijkende bestandsnamen. Ook kunnen nagenoeg identieke documenten worden gevonden, hierbij blijft de vraag wanneer een nagenoeg identiek document verwijderd kan worden uit een corpus en wanneer niet. Hiervoor bestaan geen duidelijke richtlijnen.\n",
    "\n",
    "Om het script te laten werken, is het noodzakelijk om de Natural Language Toolkit (NLTK), Numpy, Pandas, Scipy, en Scikit-Learn te installeren. Deze kunnen via de prompt geïnstalleerd worden met de onderstaande commandos:\n",
    "\n",
    "- `pip install nltk`\n",
    "- `pip install numpy`\n",
    "- `pip install pandas`\n",
    "- `pip install scipy`\n",
    "- `pip install -U scikit-learn`\n",
    "\n",
    "Documentatie:\n",
    "- NLTK: https://www.nltk.org/index.html\n",
    "- Numpy: https://numpy.org/\n",
    "- Pandas: https://pandas.pydata.org/\n",
    "- Scipy: https://scipy.org/ \n",
    "- Scikit-learn: https://scikit-learn.org/stable/install.html\n",
    "\n",
    "Voor meer informatie over de cosinusgelijkenis, zie: https://www.youtube.com/watch?v=XartD5Z4XZM\n",
    "\n",
    "Dit script komt uit: Aman Kedia & Mayank Rasu (2020) *Hands-On Python Natural Language Processing*, p. 92-95 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stap 1: Importeren van de benodigde python biblioteken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stap 2: Laden van het corpus\n",
    "\n",
    "Om dit script te laten draaien, zijn twee onderdelen nodig. Deze twee onderdelen worden gecreëerd in de onderstaande celblokjes. Ten eerste moeten we een corpus definiëren met de teksten uit de json. Ten tweede moeten we een index samenstellen met de bestandsnamen, zodat deze later uit het corpus kunnen worden gehaald als de identieke of nagenoeg identieke documenten zijn geïdentificeerd. Het is dus belangrijk om te checken of de documentnamen corresponderen met de tekst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#teksten uit de json laden\n",
    "filepath = '' #VUL IN: plaats tussen de aanhalingstekens het pad naar het json-bestand dat je in Hst 3 gemaakt hebt\n",
    "\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "                                                                 \n",
    "texts = load_data(filepath)[\"texts\"] #lijst met alle teksten uit de json \n",
    "doc_id = load_data(filepath)[\"doc_id\"] #lijst met alle bestandsnamen uit de json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus samenstellen\n",
    "corpus = pd.Series(texts, index=[doc_id]) #samenvoegen van lijst van teksten en index\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Stap 3: preprocessing van de data\n",
    "\n",
    "In de onderstaande celblokjes wordt de benodige preprocessing oftewel corpusopschoning uitgevoerd. De corpusopschoning zorgt ervoor dat de teksten kleiner worden en de berekening van de cosinusgelijkenissen minder vraagt van de computer. In de blokjes hoeft niets te worden aangepast of ingevuld, zolang er met behulp van de bovenstaande celblokjes eerst een corpus is samengesteld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In de onderstaande celblokjes worden verschillende pre-processing taken gedefinieerd, die later tegelijkertijd toegepast kunnen worden\n",
    "\n",
    "def text_clean(corpus, keep_list):\n",
    "   \n",
    "    cleaned_corpus = pd.Series()\n",
    "    for row in corpus:\n",
    "        qs = []\n",
    "        for word in row.split():\n",
    "            if word not in keep_list:\n",
    "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
    "                p1 = p1.lower()\n",
    "                qs.append(p1)\n",
    "            else : qs.append(word)\n",
    "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(corpus):\n",
    "    lem = WordNetLemmatizer()\n",
    "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(corpus, stem_type = None):\n",
    "    if stem_type == 'snowball':\n",
    "        stemmer = SnowballStemmer(language = 'english')\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    else :\n",
    "        stemmer = PorterStemmer()\n",
    "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removal(corpus):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functie waarmee de bovenstaande preprocessing taken tegelijk kunnen worden uitgevoerd (opschoning, stemming of lemmatization, verwijderen van stopwoorden) \n",
    "\n",
    "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
    "    \n",
    "    #middels de Boolean variabelen kan een keuze worden gemaakt om een bepaalde taak wel of niet uit te voeren. Er moet een keuze worden gemaakt \n",
    "    #tussen stemming en lemmatization. Allebei toepassen is nutteloos. Lemmatization heeft de voorkeur omdat hierbij ook rekening wordt gehouden \n",
    "    #met de functie/plaats van een woord in de zin.\n",
    "    \n",
    "    if cleaning == True:\n",
    "        corpus = text_clean(corpus, keep_list)\n",
    "    \n",
    "    if remove_stopwords == True:\n",
    "        corpus = stopwords_removal(corpus)\n",
    "    else:\n",
    "        corpus = [[x for x in x.split()] for x in corpus]\n",
    "    \n",
    "    if lemmatization == True:\n",
    "        corpus = lemmatize(corpus)   \n",
    "        \n",
    "    if stemming == True:\n",
    "        corpus = stem(corpus, stem_type)\n",
    "    \n",
    "    corpus = [' '.join(x) for x in corpus]        \n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing van het corpus met gebruik van lemmatisatie en het verwijderen van stopwoorden (lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming = False, stem_type = None,\n",
    "                                lemmatization = True, remove_stopwords = True)\n",
    "preprocessed_corpus[0] #eerste tekst uit tekst wordt geprint ter controle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stap 4: Berekenen van de cosinusgelijkenis met TFIDF document vectors \n",
    "\n",
    "Nu het corpus is opgeschoond, kan de cosinusgelijkenis berekend worden. Dit wordt in de onderstaande celblokjes gedaan door eerst de documenten als vector te representeren en vervolgens de cosinusgelijkenis tussen elk document te berekenen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hier printen we alleen de cosinusgelijkenis voor documenten die identiek zijn en documenten die voor tussen 95% en 99% procent overeenkomen, \n",
    "#zie het volgende celblokje om alle cosinusgelijkeniswaardes tussen alle documenten te printen\n",
    "\n",
    "j_list = [] \n",
    "\n",
    "for i in range(tf_idf_matrix.shape[0]):\n",
    "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
    "        if cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]) >= 1:\n",
    "            j_list.append(j)\n",
    "            print(\"De cosinusgelijkenis tussen documenten \", i, \"en\", j, \"is 1\")\n",
    "        if 0.95 < cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]) < 1: #de waarde 0.95 kan eventueel worden aangepast \n",
    "            j_list.append(j)\n",
    "            print(\"De cosinusgelijkenis tussen documenten \", i, \"en\", j, \"is tussen 0.95 en 0.99\")\n",
    "\n",
    "print(j_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#hier bereken we de cosinusgelijkenis voor alle documenten. Let op: een lange lijst met waardes wordt geprint.\n",
    "for i in range(tf_idf_matrix.shape[0]):\n",
    "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
    "        print(\"De cosinusgelijkenis tussen documenten \", i, \"en\", j, \"is: \",\n",
    "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printen van alleen de identieke documenten\n",
    "\n",
    "identical_doc = []\n",
    "\n",
    "for i in range(tf_idf_matrix.shape[0]):\n",
    "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
    "        if cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]) >= 1:\n",
    "            identical_doc.append(j)\n",
    "            print(\"De cosinusgelijkenis tussen documenten \", i, \"en\", j, \"is 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stap 5: Identieke & nagenoeg identieke documenten verwijderen uit het corpus\n",
    "\n",
    "De uitkomst van het vorige celblokje kan nu gebruikt worden om een nieuw corpus te creeëren zonder de identieke en nagenoeg identieke documenten. Hiervoor moet de index en de lijst met originele tekst-bestanden 'getrimd' worden. Als alleen identieke documenten verwijderd moeten worden, vervang dan in de onderstaande celblokjes de lijst j_list met de lijst identical_doc en de lijst j_list1 met de lijst identical_doc1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#herhalingen uit lijst halen, we willen elk indexnummer slechts een keer in de lijst hebben\n",
    "j_list1 = []\n",
    "\n",
    "for i in j_list:\n",
    "    if i not in j_list1:\n",
    "        j_list1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_multiple_element(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)\n",
    "\n",
    "#waardes van j_list1 uit index verwijderen\n",
    "delete_multiple_element(doc_id, j_list1)\n",
    "#waardes van j_list1 uit lijst met documenten verwijderen\n",
    "delete_multiple_element(texts, j_list1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#controleer of de lengte van documents en index overeenkomen\n",
    "print(len(doc_id))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stap 6: Nieuwe json samenstellen zonder identieke en nagenoeg identieke bestanden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = '' + '/' #VUL IN: plaats tussen de eerste aanhalingstekens het pad naar de map waar je het json-bestand wilt bewaren\n",
    "filename = '' + '.json' #VUL IN: plaats tussen de eerste aanhalingstekens de bestandsnaam die je aan het json-bestand wilt geven\n",
    "\n",
    "lists = ['doc_id', 'texts']\n",
    "\n",
    "data = {listname: globals()[listname] for listname in lists}\n",
    "with open(path + filename, 'w') as outfile:  \n",
    "    json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Kopiëren, verplaatsen of verwijderen van bepaalde bestanden met glob en shutil uit archief op basis van cosinusgelijkenis\n",
    "\n",
    "Met de onderstaande cel blokjes kunnen de identieke en nagenoeg identieke bestanden uit het corpus worden gekopieerd (shutil.copy), verplaatst (shutil.move) of verwijderd (os.remove) worden. Hiervoor moeten we alle bestandsnamen uit de index verwijderen die we willen behouden.\n",
    "\n",
    "Bronvermelding: https://thispointer.com/python-how-to-remove-files-by-matching-pattern-wildcards-certain-extensions-only/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nieuwe index samenstellen met alle bestandsnamen\n",
    "index2 = load_data(filepath)[\"doc_id\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_delete = sorted(j_list1) #vervang j_list1 met identical_doc1 als alleen identieke documenten verplaats, gekopieerd of verwijderd moeten worden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_all = list(range(X)) #VUL IN: vervang de X voor het totaal aantal bestanden in het corpus\n",
    "files_keep = []\n",
    "\n",
    "for i in files_all:\n",
    "    if not i in files_delete:\n",
    "        files_keep.append(i)\n",
    "        \n",
    "print(len(files_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verwijderen van files_keep uit de index, deze willen we namelijk behouden in het archief \n",
    "def delete_multiple_element(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)\n",
    "            \n",
    "delete_multiple_element(index2, files_keep)\n",
    "print(len(index2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met het onderstaande script kun je de uit het archief te verwijderen bestanden kopiëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.copy = kopiëren van bestanden\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "path_copy = '' #VUL IN: plaats tussen de aanhalingstekens het pad naar de hoofdmap van het archief\n",
    "\n",
    "dst_folder_copy = '' + '/' #VUL IN: plaats tussen de eerste aanhalingstekens het pad van de map waarnaar de bestanden gekopieerd moeten worden\n",
    "\n",
    "for item in index2:\n",
    "    files = glob.glob(path_copy + f\"/**/{item.title()}\", recursive = True)\n",
    "    \n",
    "    for file in files:\n",
    "        file_name_copy = os.path.basename(file)\n",
    "        shutil.copy(file, dst_folder_copy + file_name_copy) \n",
    "        print('Gekopieerd naar:', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met het onderstaande script kun je de uit het archief te verwijderen bestanden verplaatsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shutil.move = verplaatsen van bestanden\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "path_move = '' #VUL IN: plaats tussen de aanhalingstekens het pad naar de hoofdmap van het archief\n",
    "\n",
    "dst_folder_move = '' + '/' #VUL IN: plaats tussen de eerste aanhalingstekens het pad naar de map waarnaar je de bestanden wilt verplaatsen\n",
    "\n",
    "for item in index2:\n",
    "    files = glob.glob(path_move + f\"/**/{item.title()}\", recursive = True)\n",
    "    \n",
    "    for file in files:\n",
    "        file_name_move = os.path.basename(file)\n",
    "        shutil.move(file, dst_folder_move + file_name_move) \n",
    "        print('Verplaatst naar:', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met het onderstaande script kun je de uit het archief te verwijderen bestanden verwijderen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.remove = let op: met os.remove worden bestanden verwijderd \n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "path_delete = '' #VUL IN: plaats tussen de aanhalingstekens het pad naar de hoofdmap van het archief\n",
    "\n",
    "for item in index2:\n",
    "    files = glob.glob(path_delete + f\"/**/{item.title()}\", recursive = True)\n",
    "    \n",
    "    for file in files:\n",
    "        file_name = os.path.basename(file)\n",
    "        os.remove(file)\n",
    "        print('Verwijderd:', file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
